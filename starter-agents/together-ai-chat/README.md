# Together AI Chat

A high-performance chat application using Together AI's inference platform, offering access to multiple open-source models with fast inference and competitive pricing.

## üåü Features

- **Multiple Open Models**: Llama 2/3, Mistral, CodeLlama, and more
- **Fast Inference**: Optimized infrastructure for quick responses
- **Cost-Effective**: Competitive pricing for high-volume usage
- **Model Comparison**: Side-by-side comparison of different models
- **Streaming Responses**: Real-time response generation
- **Custom Parameters**: Fine-tune temperature, top-p, and other settings

## üõ†Ô∏è Tech Stack

- **AI Platform**: Together AI inference API
- **Frontend**: Streamlit with real-time streaming
- **Language**: Python 3.8+
- **Models**: Llama, Mistral, CodeLlama, and other open-source models

## üöÄ Quick Start

1. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Set up environment**:
   ```bash
   cp .env.example .env
   # Add your Together AI API key
   ```

3. **Run the application**:
   ```bash
   streamlit run app.py
   ```

## üí° Available Models

### Code Generation
- **CodeLlama 34B**: Advanced code generation and completion
- **CodeLlama 13B**: Balanced performance for coding tasks
- **CodeLlama 7B**: Fast code assistance

### General Chat
- **Llama 2 70B**: Most capable for complex reasoning
- **Llama 2 13B**: Balanced performance and speed
- **Mistral 7B**: Fast and efficient responses

### Specialized Models
- **Alpaca**: Instruction-following fine-tuned model
- **Vicuna**: Conversational AI optimized model

## üéØ Use Cases

- **Code Development**: Programming assistance and code review
- **Content Creation**: Writing and editing assistance
- **Research**: Information synthesis and analysis
- **Education**: Tutoring and explanation of concepts
- **Prototyping**: Quick AI integration for testing ideas
